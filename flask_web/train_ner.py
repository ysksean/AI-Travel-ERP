import json
import os
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, ElectraForTokenClassification
from torch.optim import AdamW

# --- 1. ì„¤ì • ë° íƒœê·¸ ì •ì˜ (ERP í¼ êµ¬ì¡°ì™€ 1:1 ë§¤í•‘) ---
EPOCHS = 5
LEARNING_RATE = 5e-5
BATCH_SIZE = 2
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, '../models')
DATA_FILE = os.path.join(BASE_DIR, 'train_data.json')

# [ì¤‘ìš”] ì„¸ë¶„í™”ëœ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ (ì´ 31ê°œ)
LABEL_LIST = [
    "O",
    "B-HOTEL_NAME", "I-HOTEL_NAME", "B-HOTEL_GRADE", "I-HOTEL_GRADE", "B-HOTEL_LOC", "I-HOTEL_LOC",
    "B-GOLF_NAME", "I-GOLF_NAME", "B-GOLF_OP", "I-GOLF_OP",
    "B-FLIGHT_NAME", "I-FLIGHT_NAME", "B-FLIGHT_NUM", "I-FLIGHT_NUM", "B-DEPART_TIME", "I-DEPART_TIME",
    "B-PRICE", "I-PRICE", "B-INCLUSION", "I-INCLUSION", "B-EXCLUSION", "I-EXCLUSION",
    "B-REFUND", "I-REFUND", "B-DATE", "I-DATE", "B-CITY", "I-CITY", "B-NOTE", "I-NOTE"
]
LABEL2ID = {label: i for i, label in enumerate(LABEL_LIST)}


class NERDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=128):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        item = self.data[index]
        text = item['text']
        labels = item['labels']

        words = text.split()
        token_list = []
        label_ids = []

        for word, label in zip(words, labels):
            word_tokens = self.tokenizer.tokenize(word)
            if not word_tokens: continue
            token_list.extend(word_tokens)
            try:
                label_ids.append(LABEL2ID[label])
            except KeyError:
                label_ids.append(0)  # ëª¨ë¥´ëŠ” ë¼ë²¨ì€ 'O' ì²˜ë¦¬
            label_ids.extend([-100] * (len(word_tokens) - 1))

        encoding = self.tokenizer.encode_plus(
            token_list, max_length=self.max_len, padding='max_length',
            truncation=True, is_split_into_words=True, return_tensors='pt'
        )

        pad_len = self.max_len - len(label_ids)
        if pad_len > 0:
            label_ids += [-100] * pad_len
        else:
            label_ids = label_ids[:self.max_len]

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label_ids, dtype=torch.long)
        }


def train():
    print("ğŸš€ ê³ ë„í™” ëª¨ë¸ í•™ìŠµ ì¤€ë¹„ ì¤‘...")

    # ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    ner_save_path = os.path.join(MODEL_DIR, 'koelectra_ner')
    if not os.path.exists(ner_save_path): os.makedirs(ner_save_path)

    tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")

    # [í•µì‹¬] num_labelsë¥¼ 31ê°œë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë¸ ì´ˆê¸°í™”
    model = ElectraForTokenClassification.from_pretrained(
        "monologg/koelectra-base-v3-discriminator",
        num_labels=len(LABEL_LIST)
    )

    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)

    dataset = NERDataset(raw_data, tokenizer)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

    print(f"ğŸ”¥ í•™ìŠµ ì‹œì‘! (Device: {device}, Labels: {len(LABEL_LIST)})")

    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        for batch in loader:
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"  Epoch {epoch + 1}/{EPOCHS} - Loss: {total_loss / len(loader):.4f}")

    model.save_pretrained(ner_save_path)
    tokenizer.save_pretrained(os.path.join(MODEL_DIR, 'tokenizer'))
    print(f"\nğŸ‰ í•™ìŠµ ì™„ë£Œ! ê³ ë„í™”ëœ ëª¨ë¸ì´ '{ner_save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    train()